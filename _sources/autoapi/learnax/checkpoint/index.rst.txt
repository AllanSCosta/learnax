learnax.checkpoint
==================

.. py:module:: learnax.checkpoint


Classes
-------

.. autoapisummary::

   learnax.checkpoint.Checkpointer


Module Contents
---------------

.. py:class:: Checkpointer(registry_path: str, run_id: str, save_every: int = 300, checkpoint_every: int = 5000, max_to_keep: Union[int, float] = None, keep_best: bool = False, metric_name: str = 'val/loss', minimize_metric: bool = True, prepare_fn: Callable = None)

   A class for managing model checkpoints during training.

   This class handles the saving, loading, and management of checkpoints,
   providing a simple interface that abstracts away the details of file management,
   serialization, and device handling.

   .. attribute:: registry_path

      Base directory for saving checkpoints

      :type: str

   .. attribute:: run_id

      Identifier for the current run

      :type: str

   .. attribute:: checkpoint_dir

      Full path to the checkpoint directory

      :type: str

   .. attribute:: save_every

      Steps between saving regular checkpoints

      :type: int

   .. attribute:: checkpoint_every

      Steps between saving numbered checkpoints

      :type: int

   .. attribute:: max_to_keep

      Maximum number of numbered checkpoints to retain

      :type: int

   .. attribute:: keep_best

      Whether to retain the best checkpoint based on a metric

      :type: bool

   .. attribute:: metric_name

      Name of the metric to use for best checkpoint selection

      :type: str

   .. attribute:: minimize_metric

      Whether the metric should be minimized (True) or maximized (False)

      :type: bool

   .. attribute:: best_metric_value

      Current best metric value

      :type: float

   .. attribute:: prepare_fn

      Function to prepare state for checkpointing

      :type: Callable


   .. py:attribute:: registry_path


   .. py:attribute:: run_id


   .. py:attribute:: checkpoint_dir


   .. py:attribute:: save_every
      :value: 300



   .. py:attribute:: checkpoint_every
      :value: 5000



   .. py:attribute:: max_to_keep
      :value: None



   .. py:attribute:: keep_best
      :value: False



   .. py:attribute:: metric_name
      :value: 'val/loss'



   .. py:attribute:: minimize_metric
      :value: True



   .. py:attribute:: best_metric_value


   .. py:attribute:: prepare_fn


   .. py:method:: should_save_latest(step: int) -> bool

      Check if the latest checkpoint should be saved at the current step.

      :param step: Current training step
      :type step: int

      :returns: True if the latest checkpoint should be saved, False otherwise
      :rtype: bool



   .. py:method:: should_save_numbered(step: int) -> bool

      Check if a numbered checkpoint should be saved at the current step.

      :param step: Current training step
      :type step: int

      :returns: True if a numbered checkpoint should be saved, False otherwise
      :rtype: bool



   .. py:method:: save_checkpoint(train_state: Any, step: int = None, metrics: Dict[str, float] = None, is_best: bool = False, suffix: str = None) -> str

      Save a checkpoint with the given training state.

      :param train_state: Training state to save
      :type train_state: Any
      :param step: Current step. If None, extracted from train_state.
      :type step: int, optional
      :param metrics: Current metrics to save with checkpoint
      :type metrics: Dict[str, float], optional
      :param is_best: Whether this is marked as the best checkpoint
      :type is_best: bool
      :param suffix: Optional suffix to add to the checkpoint filename
      :type suffix: str, optional

      :returns: Path to the saved checkpoint, or None if saving failed
      :rtype: str



   .. py:method:: save_latest(train_state: Any, step: int = None, metrics: Dict[str, float] = None) -> str

      Save the latest checkpoint.

      :param train_state: Training state to save
      :type train_state: Any
      :param step: Current step
      :type step: int, optional
      :param metrics: Current metrics
      :type metrics: Dict[str, float], optional

      :returns: Path to the saved checkpoint, or None if saving failed
      :rtype: str



   .. py:method:: update_best_checkpoint(train_state: Any, metric_value: float, step: int = None) -> bool

      Update the best checkpoint if the given metric value is better.

      :param train_state: Training state to save
      :type train_state: Any
      :param metric_value: Value of the metric to compare
      :type metric_value: float
      :param step: Current step
      :type step: int, optional

      :returns: True if a new best checkpoint was saved, False otherwise
      :rtype: bool



   .. py:method:: maybe_save_checkpoints(train_state: Any, step: int, metrics: Dict[str, float] = None) -> Dict[str, str]

      Save checkpoints if appropriate based on current step and configuration.

      This method handles saving both latest and numbered checkpoints according
      to the configured intervals, and also updates the best checkpoint if needed.

      :param train_state: Training state to save
      :type train_state: Any
      :param step: Current training step
      :type step: int
      :param metrics: Current metrics
      :type metrics: Dict[str, float], optional

      :returns: Paths to saved checkpoints by type
      :rtype: Dict[str, str]



   .. py:method:: load_checkpoint(path_or_index: Union[str, int]) -> Dict[str, Any]


   .. py:method:: load_latest() -> Dict[str, Any]

      Load the latest checkpoint.

      :returns: Loaded checkpoint data, or None if loading failed
      :rtype: Dict[str, Any]



   .. py:method:: load_best() -> Dict[str, Any]

      Load the best checkpoint.

      :returns: Loaded checkpoint data, or None if loading failed
      :rtype: Dict[str, Any]



   .. py:method:: list_available_checkpoints() -> Dict[str, str]

      List all available checkpoints.

      :returns: Dictionary of available checkpoint types and their paths
      :rtype: Dict[str, str]



